from flask import Flask, request, jsonify
from flask_cors import CORS
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

app = Flask(__name__)
CORS(app)

# Cat치logo de autos
auto_info = {
    "chevrolet spark": {
        "descripcion": "Auto compacto, ideal para ciudad. Muy econ칩mico y f치cil de parquear.",
        "imagen": "https://acnews.blob.core.windows.net/imgnews/medium/NAZ_1717278aeb7e4c4f9a37b6a90b173a6d.jpg"
    },
    "toyota prado": {
        "descripcion": "SUV robusta, potente, perfecta para viajes largos y terrenos dif칤ciles.",
        "imagen": "https://toyotadecolombia.com.co/wp-content/uploads/2021/07/prado-2021.png"
    },
    "mazda cx-30": {
        "descripcion": "Crossover moderno, elegante y c칩modo. Excelente para uso familiar.",
        "imagen": "https://mazda.com.co/wp-content/uploads/2021/11/CX-30-LUXURY.png"
    },
    "suzuki vitara": {
        "descripcion": "SUV compacta, confiable, buena para ciudad y campo.",
        "imagen": "https://suzukiauto.com.co/wp-content/uploads/2021/11/Vitara-negro.png"
    },
    "chevrolet aveo": {
        "descripcion": "Sed치n cl치sico, c칩modo, ideal para quienes buscan rendimiento a buen precio.",
        "imagen": "https://noticias.autocosmos.hn/resizer/tU-V47smuQyDjf4Yh_jYhT6trYY=/fit-in/1200x900/smart/cloudfront-us-east-1.images.arcpublishing.com/gruponacion/BYAZFFLC6VD23D7GZXK3TYYJH4.jpg"
    }
}

# Crear el men칰 en texto plano
menu_autos = ""
for nombre, datos in auto_info.items():
    menu_autos += f"- {nombre.title()}: {datos['descripcion']}\n"

# Template para LangChain
template = """
Eres un asesor profesional de ventas de autos. Alguien te har치 una pregunta espec칤fica.

Responde de forma clara y profesional, solo con la informaci칩n necesaria.
Incluye una breve descripci칩n del auto si lo mencionan y el enlace a su imagen si corresponde.

Mensaje del cliente: {input}
Tu respuesta:
"""

prompt = PromptTemplate(
    input_variables=["input"],
    template=template,
)

# Usar llama3 como LLM
llm = Ollama(model="llama3")
chain = LLMChain(llm=llm, prompt=prompt)

@app.route('/ia', methods=['POST'])
def responder():
    data = request.get_json()
    pregunta = data.get("pregunta", "").lower()

    # Detectar si es un saludo o solicitud general
    if any(p in pregunta for p in ["hola", "buenas", "cat치logo", "ver autos", "mostrar autos", "autos disponibles"]):
        respuesta = f"""춰Hola! Soy tu asesor virtual de autos 游뚱

Aqu칤 tienes nuestro cat치logo actual:
{menu_autos}

Dime si alguno te interesa o si deseas una recomendaci칩n personalizada.
"""
    else:
        respuesta = chain.run(input=pregunta)

    return jsonify({"respuesta": respuesta})

if __name__ == '__main__':
    app.run(port=5000)
